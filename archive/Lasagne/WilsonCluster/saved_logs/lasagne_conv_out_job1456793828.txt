PBS prologue
Job mnv-conv-1456793828 submitted from mic.fnal.gov started Mon Feb 29 18:57:09 CST 2016 jobid 106281.tev.fnal.gov
gpu1
PBS_O_WORKDIR is /home/perdue/ANNMINERvA/WilsonCluster
Git repo version is e7b3c054e7eb-dirty
Git repo contains uncomitted changes! Please commit your changes
before submitting a job. If you feel your changes are experimental,
just use a feature branch.

Changed files:
WilsonCluster/job_lasagne_conv_mlp.sh

Using gpu device 0: Tesla K20m (CNMeM is disabled)
/usr/local/python2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.
  warnings.warn("downsample module has been moved to the pool module.")
Starting...
minerva_triamese_lasagnefuel.py
 Begin with saved parameters? False
 Saved parameters file: ./lminervatriamese_model1456793829.npz
 Saved parameters file exists? False
 Dataset: /phihome/perdue/theano/data/nukecc_fuel_me1B_subset93k.hdf5
 Dataset size: 109183675
 Planned number of epochs: 1
 Learning rate: 0.001
 Momentum: 0.9
 L2 regularization penalty scale: 0.0001
 Batch size: 500
Loading data...
 Learning sample size = 74691 examples
 Validation sample size = 9336 examples
Convpool1 params: {'filter_size': (3, 3), 'pool_size': (2, 2), 'nfilters': 32}
Convpool2 params: {'filter_size': (3, 3), 'pool_size': (2, 2), 'nfilters': 32}

In -->         Layer    --> Out    Description                                                  
-------        -----    -------    -----------                                                  
[]             0        [1]        <lasagne.layers.input.InputLayer object at 0x2b08c4cb1750>   
[0]            1        [2]        <lasagne.layers.conv.Conv2DLayer object at 0x2b08c4cb1850>   
[1]            2        [3]        <lasagne.layers.pool.MaxPool2DLayer object at 0x2b08c4cb1810>
[2]            3        [4]        <lasagne.layers.conv.Conv2DLayer object at 0x2b08a122a850>   
[3]            4        [5]        <lasagne.layers.pool.MaxPool2DLayer object at 0x2b08c4cb1890>
[4]            5        [6]        <lasagne.layers.noise.DropoutLayer object at 0x2b08a122a810> 
[5]            6        [21]       <lasagne.layers.dense.DenseLayer object at 0x2b08a122aad0>   
[]             7        [8]        <lasagne.layers.input.InputLayer object at 0x2b08c4cb1790>   
[7]            8        [9]        <lasagne.layers.conv.Conv2DLayer object at 0x2b08a123b1d0>   
[8]            9        [10]       <lasagne.layers.pool.MaxPool2DLayer object at 0x2b08a122ab50>
[9]            10       [11]       <lasagne.layers.conv.Conv2DLayer object at 0x2b08a123b490>   
[10]           11       [12]       <lasagne.layers.pool.MaxPool2DLayer object at 0x2b08a123b190>
[11]           12       [13]       <lasagne.layers.noise.DropoutLayer object at 0x2b08a123b450> 
[12]           13       [21]       <lasagne.layers.dense.DenseLayer object at 0x2b08a123b710>   
[]             14       [15]       <lasagne.layers.input.InputLayer object at 0x2b08c4cb17d0>   
[14]           15       [16]       <lasagne.layers.conv.Conv2DLayer object at 0x2b08a123ba50>   
[15]           16       [17]       <lasagne.layers.pool.MaxPool2DLayer object at 0x2b08a123b790>
[16]           17       [18]       <lasagne.layers.conv.Conv2DLayer object at 0x2b08a123bd10>   
[17]           18       [19]       <lasagne.layers.pool.MaxPool2DLayer object at 0x2b08a123ba10>
[18]           19       [20]       <lasagne.layers.noise.DropoutLayer object at 0x2b08a123bcd0> 
[19]           20       [21]       <lasagne.layers.dense.DenseLayer object at 0x2b08a123bf90>   
[6, 13, 20]    21       [22]       <lasagne.layers.merge.ConcatLayer object at 0x2b08a123bf50>  
[21]           22       [23]       <lasagne.layers.noise.DropoutLayer object at 0x2b08a1248310> 
[22]           23       []         <lasagne.layers.dense.DenseLayer object at 0x2b08a1248350>   

        ////
        Use AdaGrad update schedule for learning rate, see Duchi, Hazan, and
        Singer (2011) "Adaptive subgradient methods for online learning and
        stochasitic optimization." JMLR, 12:2121-2159
        ////
        

        ////
        Apply Nesterov momentum using Lisa Lab's modifications.
        ////
        
Starting training...
[(0, 10000), (10000, 20000), (20000, 30000), (30000, 40000), (40000, 50000), (50000, 60000), (60000, 70000), (70000, 74691)]
(0, 10000)
(10000, 20000)
(20000, 30000)
(30000, 40000)
(40000, 50000)
(50000, 60000)
(60000, 70000)
(70000, 74691)
Epoch 1 of 1 took 16.927s
  training loss:		1.222311
  validation loss:		1.162886
  validation accuracy:		65.67 %
Finished 1 epochs.
Using gpu device 0: Tesla K20m (CNMeM is disabled)
/usr/local/python2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.
  warnings.warn("downsample module has been moved to the pool module.")
Starting...
minerva_triamese_lasagnefuel.py
 Begin with saved parameters? False
 Saved parameters file: ./lminervatriamese_model1456793829.npz
 Saved parameters file exists? True
 Dataset: /phihome/perdue/theano/data/nukecc_fuel_me1B_subset93k.hdf5
 Dataset size: 109183675
 Planned number of epochs: 1
 Learning rate: 0.001
 Momentum: 0.9
 L2 regularization penalty scale: 0.0001
 Batch size: 500
Loading data for prediction...
Convpool1 params: {'filter_size': (3, 3), 'pool_size': (2, 2), 'nfilters': 32}
Convpool2 params: {'filter_size': (3, 3), 'pool_size': (2, 2), 'nfilters': 32}
Final results:
  test loss:			1.153854
  test accuracy:		65.92 %
   target 1 accuracy:			87.524 %
   target 2 accuracy:			76.634 %
   target 3 accuracy:			72.392 %
   target 4 accuracy:			67.468 %
   target 5 accuracy:			55.875 %
PBS epilogue
